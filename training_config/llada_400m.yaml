env:
  HF_ENDPOINT: https://hf-mirror.com
  HF_HOME: "{repo_dir}/hf_cache"
  TRANSFORMERS_CACHE: "{repo_dir}/hf_cache/transformers"
  HF_DATASETS_CACHE: "{repo_dir}/hf_cache/datasets"
  HUGGINGFACE_HUB_CACHE: "{repo_dir}/hf_cache/hub"
  PIP_INDEX_URL: https://pypi.tuna.tsinghua.edu.cn/simple

main_args:
  model_name_or_path: answerdotai/ModernBERT-base
  dataset_name: finefineweb
  validation_dataset_name: none
  output_dir: "{repo_dir}/output/llada_400m_dl"
  config_path: "{repo_dir}/model_config/llada_400m.json"
  mode: llada

  num_train_epochs: 1
  learning_rate: 0.0003

  per_device_train_batch_size: 6
  per_device_eval_batch_size: 6
  gradient_accumulation_steps: 2

  max_length: 1024
  mlm_start_prob: 0.3
  mlm_end_prob: 0.15
  mlm_schedule_type: cosine
  warmup_ratio: 0.05

  logging_steps: 100
  save_steps: 3500
  eval_steps: 1000
  save_total_limit: 3

  seed: 42
  dataloader_num_workers: 8
  disable_tqdm: true

  generation_interval: 2000
  generation_max_new_tokens: 128
  generation_num_diffusion_steps: 64
  generation_temperature: 1.0
  generation_block_size: 64
  generation_decode_top_k: 0
  generation_remask_strategy: low_confidence
  generation_prompts:
    - "One day"

  resume_from_checkpoint: null

flags:
  - bf16

ensure_dirs:
  - "{output_dir}"
  - "{repo_dir}/hf_cache"
  - "{repo_dir}/hf_cache/transformers"
  - "{repo_dir}/hf_cache/datasets"
  - "{repo_dir}/hf_cache/hub"

extra_args: []
