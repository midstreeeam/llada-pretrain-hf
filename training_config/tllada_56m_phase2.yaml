env:
  HF_ENDPOINT: https://hf-mirror.com
  HF_HOME: "{repo_dir}/hf_cache"
  TRANSFORMERS_CACHE: "{repo_dir}/hf_cache/transformers"
  HF_DATASETS_CACHE: "{repo_dir}/hf_cache/datasets"
  HUGGINGFACE_HUB_CACHE: "{repo_dir}/hf_cache/hub"
  PIP_INDEX_URL: https://pypi.tuna.tsinghua.edu.cn/simple
  TOKENIZERS_PARALLELISM: "false"

main_args:
  model_name_or_path: "{repo_dir}/tokenizers/tinystory2"
  dataset_name: tinystories
  validation_dataset_name: none
  output_dir: "{repo_dir}/output/tllada_56m_phase2"
  config_path: "{repo_dir}/model_config/tllada_56m.json"
  mode: llada

  # Phase-2 schedule: Semi-Autoregressive Refinement
  num_train_epochs: 3
  learning_rate: 0.0002
  per_device_train_batch_size: 64
  per_device_eval_batch_size: 64
  gradient_accumulation_steps: 1
  max_length: 512

  # Semi-Autoregressive Parallel Strategy (Fast Training)
  masking_strategy: semi_autoregressive_parallel
  block_size: 2

  warmup_ratio: 0.02

  logging_steps: 100
  save_steps: 20000
  eval_steps: 2500
  save_total_limit: 3
  seed: 42
  dataloader_num_workers: 2
  disable_tqdm: true

  generation_interval: 2500
  generation_max_new_tokens: 64
  generation_num_diffusion_steps: 32
  generation_temperature: 1.0
  generation_block_size: 2
  generation_decode_top_k: 0
  generation_remask_strategy: low_confidence
  generation_prompts:
    - "Once upon a time,"
    - "In a quiet village by the sea,"

  # Point this to your Phase 1 checkpoint when available
  init_model_from_checkpoint: "{repo_dir}/output/tllada_56m_dl/checkpoint-264968"
  resume_from_checkpoint: null

  save_safetensors: false

flags:
  - bf16

ensure_dirs:
  - "{output_dir}"
  - "{repo_dir}/hf_cache"
  - "{repo_dir}/hf_cache/transformers"
  - "{repo_dir}/hf_cache/datasets"
  - "{repo_dir}/hf_cache/hub"

extra_args: []

