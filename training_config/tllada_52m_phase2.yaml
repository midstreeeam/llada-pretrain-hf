env:
  HF_ENDPOINT: https://hf-mirror.com
  HF_HOME: "{repo_dir}/hf_cache"
  TRANSFORMERS_CACHE: "{repo_dir}/hf_cache/transformers"
  HF_DATASETS_CACHE: "{repo_dir}/hf_cache/datasets"
  HUGGINGFACE_HUB_CACHE: "{repo_dir}/hf_cache/hub"
  PIP_INDEX_URL: https://pypi.tuna.tsinghua.edu.cn/simple
  TOKENIZERS_PARALLELISM: "false"

main_args:
  model_name_or_path: "{repo_dir}/tokenizers/tinystory"
  dataset_name: tinystories
  validation_dataset_name: none
  output_dir: "{repo_dir}/output/tllada_52m_phase2"
  config_path: "{repo_dir}/model_config/tllada_52m.json"
  mode: llada

  # Phase-2 schedule: start from a lower LR and shorter sequences for story focus
  num_train_epochs: 3
  learning_rate: 0.0002
  per_device_train_batch_size: 64
  per_device_eval_batch_size: 64
  gradient_accumulation_steps: 1
  max_length: 512

  # Keep current MLM settings (collator uses uniform per-sample p)
  mlm_start_prob: 0.3
  mlm_end_prob: 0.15
  mlm_schedule_type: cosine
  mlm_target_rate: 0.8
  mlm_rate_concentration: 2.5
  warmup_ratio: 0.02

  logging_steps: 100
  save_steps: 200
  eval_steps: 1000
  save_total_limit: 3
  seed: 42
  dataloader_num_workers: 2
  disable_tqdm: true

  # More frequent previews can help monitor refinement
  generation_interval: 2500
  generation_max_new_tokens: 64
  generation_num_diffusion_steps: 32
  generation_temperature: 1.0
  generation_block_size: 8
  generation_decode_top_k: 0
  generation_remask_strategy: low_confidence
  generation_prompts:
    - "Once upon a time,"
    - "In a quiet village by the sea,"

  # 从阶段一的10 epoch checkpoint加载模型权重；不恢复优化器/调度器（使用新日程）
  init_model_from_checkpoint: "{repo_dir}/output/tllada_52m_dl/checkpoint-331210"
  resume_from_checkpoint: null

  save_safetensors: false

flags:
  - bf16
  # Do not use safetensors saving for tied weights; omit the flag (default false)

ensure_dirs:
  - "{output_dir}"
  - "{repo_dir}/hf_cache"
  - "{repo_dir}/hf_cache/transformers"
  - "{repo_dir}/hf_cache/datasets"
  - "{repo_dir}/hf_cache/hub"

extra_args: []
