env:
  HF_ENDPOINT: https://hf-mirror.com
  HF_HOME: "{repo_dir}/hf_cache"
  TRANSFORMERS_CACHE: "{repo_dir}/hf_cache/transformers"
  HF_DATASETS_CACHE: "{repo_dir}/hf_cache/datasets"
  HUGGINGFACE_HUB_CACHE: "{repo_dir}/hf_cache/hub"
  PIP_INDEX_URL: https://pypi.tuna.tsinghua.edu.cn/simple

main_args:
  model_name_or_path: answerdotai/ModernBERT-base
  dataset_name: tinystories
  validation_dataset_name: none
  output_dir: "{repo_dir}/output/llada_100m_tinystories_autodl"
  config_path: "{repo_dir}/model_config/llada_100m.json"
  mode: llada
  num_train_epochs: 4
  learning_rate: 0.0003
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 1
  max_length: 512
  mlm_start_prob: 0.3
  mlm_end_prob: 0.15
  mlm_schedule_type: cosine
  warmup_ratio: 0.05
  logging_steps: 100
  save_steps: 20000
  eval_steps: 1000
  save_total_limit: 3
  seed: 42
  dataloader_num_workers: 1
  disable_tqdm: true
  generation_interval: 2000
  generation_max_new_tokens: 64
  generation_num_diffusion_steps: 16
  generation_temperature: 1.0
  generation_block_size: 32
  generation_decode_top_k: 0
  generation_remask_strategy: low_confidence
  generation_prompts:
    - "Once upon a time, a curious child asked:"
    - "In a quiet village by the sea,"
  resume_from_checkpoint: "{repo_dir}/output/llada_100m_tinystories_autodl/checkpoint-160000"

flags:
  - bf16

ensure_dirs:
  - "{output_dir}"
  - "{repo_dir}/hf_cache"
  - "{repo_dir}/hf_cache/transformers"
  - "{repo_dir}/hf_cache/datasets"
  - "{repo_dir}/hf_cache/hub"

extra_args: []
