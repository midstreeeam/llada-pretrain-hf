{
  "student_checkpoint": "output/llada_135m_ts_v/checkpoint-496815",
  "student_tokenizer": "answerdotai/ModernBERT-base",
  "judge_model": "Qwen/Qwen3-1.7B",
  "generation_tag": "gen_128_128_128",
  "timestamp": "2025-11-27T20:11:36.225998",
  "artifacts": {
    "generation_jsonl": "eval/runs/llada_135m_ts_v_ppl/gen_128_128_128/artifacts/model_generations.jsonl",
    "model_ppl_summary": "eval/runs/llada_135m_ts_v_ppl/gen_128_128_128/artifacts/model_generations_perplexity.json",
    "model_ppl_details": "eval/runs/llada_135m_ts_v_ppl/gen_128_128_128/artifacts/model_generations_perplexity.jsonl",
    "reference_ppl_summary": "eval/runs/llada_135m_ts_v_ppl/gen_128_128_128/artifacts/reference_perplexity.json",
    "reference_ppl_details": "eval/runs/llada_135m_ts_v_ppl/gen_128_128_128/artifacts/reference_perplexity.jsonl"
  },
  "generation_metadata": {
    "num_prompts": 256,
    "records_written": 256,
    "output_path": "eval/runs/llada_135m_ts_v_ppl/gen_128_128_128/artifacts/model_generations.jsonl",
    "device": "cuda",
    "dtype": "torch.float16"
  },
  "model_perplexity": {
    "total_sequences": 256,
    "total_tokens": 30777,
    "total_neg_log_likelihood": 69913.0,
    "mean_per_token_nll": 2.271598921272379,
    "perplexity": 9.694889793467802,
    "metadata": {
      "judge_model": "Qwen/Qwen3-1.7B",
      "dtype": "torch.bfloat16",
      "device": "cuda",
      "max_context_length": 512
    }
  },
  "reference_perplexity": {
    "total_sequences": 100,
    "total_tokens": 17875,
    "total_neg_log_likelihood": 37547.0,
    "mean_per_token_nll": 2.1005314685314684,
    "perplexity": 8.170511128406423,
    "metadata": {
      "judge_model": "Qwen/Qwen3-1.7B",
      "dtype": "torch.bfloat16",
      "device": "cuda",
      "max_context_length": 512
    }
  }
}